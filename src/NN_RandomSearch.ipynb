{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pickle as pk\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "split_idx = np.load('task_2_idx_split.npy', allow_pickle=True) # load pre-fixed data split indices\n",
    "embs = np.load('fp_circ.npy') #load the fingerprints\n",
    "y_names = train.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first split for now\n",
    "data = embs[split_idx[0][0],:]\n",
    "test_idx = split_idx[0][1]\n",
    "test_data = torch.Tensor(embs[test_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We removed searching for dropout rate based on the results\n",
    "class Docking_Prediction(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super().__init__()        \n",
    "        self.seq = []\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            self.seq.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "#            self.seq.append(nn.Dropout(dropout_rate, inplace=True))\n",
    "            if i!=len(layer_dims)-2:\n",
    "#                self.seq.append(nn.BatchNorm1d(layer_dims[i+1]))\n",
    "                self.seq.append(nn.LeakyReLU())   \n",
    "        self.sequential = nn.Sequential(*self.seq)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_layers(input_dim):    \n",
    "    widths= range(1, input_dim+1, 256) #this sets the maximum number of layers\n",
    "    n = np.random.randint(len(widths))\n",
    "    if n==0:\n",
    "        n+=1\n",
    "    gap = np.floor(input_dim/n)\n",
    "    layer_dims = [int(a*gap) for a in range(1,n)]\n",
    "    \n",
    "    if len(layer_dims)==1:\n",
    "        return [input_dim, 1]\n",
    "    else:\n",
    "        layer_dims.append(input_dim)\n",
    "        layer_dims.insert(0,1)\n",
    "        return layer_dims[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dropout():\n",
    "    r = [0, 0.1, 0.2]\n",
    "    n = np.random.randint(3)\n",
    "    return r[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = embs.shape[1]\n",
    "max_epoch = 100\n",
    "batch_size = 64\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=0 # Search the architecture for a column\n",
    "labels = np.array(train[y_names[col]][split_idx[0][0]])\n",
    "data_train, data_valid, label_train, label_valid=train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae = {}\n",
    "L = []\n",
    "#D = []\n",
    "tic = time.time()\n",
    "for ite in range(max_iter):\n",
    "    layer_dims = gen_layers(emb_dim)\n",
    "    dropout_rate = gen_dropout()\n",
    "    \n",
    "    L.append(layer_dims)\n",
    "#    D.append(dropout_rate)\n",
    "    print(ite, layer_dims)\n",
    "    \n",
    "    model = Docking_Prediction(layer_dims, dropout_rate)\n",
    "    model_best = Docking_Prediction(layer_dims, dropout_rate)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, \n",
    "                                                          min_lr=1e-7, verbose=True)\n",
    "    t=5\n",
    "    patience = t\n",
    "    loss_valid = []\n",
    "    loss_train = []\n",
    "    valid_error = 10e5\n",
    "    e=1e-4\n",
    "    error_best = 10e5\n",
    "\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        loss_run = 0\n",
    "        \n",
    "        n_batch = int(np.floor(data_train.shape[0]/batch_size))\n",
    "        n_batch_valid = int(np.floor(data_valid.shape[0]/batch_size))\n",
    "\n",
    "        for i in range(n_batch):\n",
    "            data_batch= torch.Tensor(data_train[i*batch_size:(i+1)*batch_size,:])\n",
    "            label_batch = torch.Tensor(label_train[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            outputs = model(data_batch)\n",
    "            loss = criterion(torch.squeeze(outputs), label_batch)\n",
    "            loss_run+=loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i%1000==0:\n",
    "                print(f'Batch({i+1}) loss: {loss}')\n",
    "\n",
    "        # calculate validation loss\n",
    "        loss_valid_batch = 0\n",
    "        for i in range(n_batch_valid):\n",
    "\n",
    "            label_batch = torch.Tensor(label_valid[i*batch_size:(i+1)*batch_size])\n",
    "            data_batch= torch.Tensor(data_valid[i*batch_size:(i+1)*batch_size,:])\n",
    "\n",
    "            outputs = model(data_batch)\n",
    "            loss = criterion(torch.squeeze(outputs), label_batch)\n",
    "            loss_valid_batch+=loss\n",
    "\n",
    "        if loss_valid_batch/n_batch_valid >= error_best-e:\n",
    "            patience-=1\n",
    "        else:\n",
    "            patience=t\n",
    "\n",
    "        if patience == 0:\n",
    "            break\n",
    "        print('patience=', patience)\n",
    "\n",
    "        valid_error=loss_valid_batch/n_batch_valid\n",
    "        loss_valid.append(valid_error)\n",
    "\n",
    "        if valid_error<error_best:\n",
    "            model_best = model\n",
    "            error_best = valid_error\n",
    "\n",
    "        scheduler.step(valid_error)\n",
    "\n",
    "        loss_run=loss_run/n_batch\n",
    "        loss_train.append(loss_run)\n",
    "        print('|epoch {:4d} | loss {:.6f} | valid loss {:.6f} |'.format(epoch, loss_run,valid_error)) \n",
    "    \n",
    "    print('------------Model fitting finished-------------')\n",
    "    \n",
    "    # Model test\n",
    "    preds_test = model_best(test_data)\n",
    "    test_label = np.array(train[y_names[col]][test_idx])\n",
    "    mae[ite] = metrics.mean_absolute_error(test_label, preds_test.detach().numpy())\n",
    "\n",
    "    \n",
    "    if min(mae, key=mae.get) == ite:\n",
    "        torch.save(model_best.state_dict(), 'BestNN_circ/bestNN_circ_col{}'.format(col))\n",
    "        print('!!!!!!!!!!!!!!saving best model!!!!!!!!!!!!!!')\n",
    "        \n",
    "pk.dump(L, open('layer_dims_col{}.pk'.format(col), 'wb'))\n",
    "#pk.dump(D, open('dropout_col{}.pk'.format(col), 'wb'))\n",
    "\n",
    "pk.dump(mae, open('mae_NN_circ_col{}.pk'.format(col), 'wb'))\n",
    "toc=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_arch = {}\n",
    "for i in range(18):\n",
    "    mae = pk.load('mae_NN_circ_col{}.pk'.format(i), 'rb')\n",
    "    L = pk.load('layer_dims_col{}.pk'.format(col), 'wb')\n",
    "    NN_arch[col] = L[min(mae, key=mae.get)]\n",
    "pk.dump(NN_arch, open('NN_arch.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_arch = pk.load(open('NN_arch.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3CLPro_pocket1 0.7455625983793647\n",
      "ADRP-ADPR_pocket1 0.3468090994142603\n",
      "ADRP-ADPR_pocket5 0.35057756812916857\n",
      "ADRP_pocket1 0.2632114434953089\n",
      "ADRP_pocket12 0.26126620909567233\n",
      "ADRP_pocket13 0.29410418168085595\n",
      "COV_pocket1 0.20552249665315506\n",
      "COV_pocket2 0.20617000194566118\n",
      "COV_pocket8 0.3156156268186922\n",
      "COV_pocket10 0.31273062214992664\n",
      "NSP9_pocket2 0.3541901033060638\n",
      "NSP9_pocket7 0.28077987337492133\n",
      "NSP15_pocket1 0.3129642783366309\n",
      "ORF7A_pocket2 0.3016622075907389\n",
      "PLPro_chainA_pocket3 0.3250212305640291\n",
      "PLPro_chainA_pocket23 0.39383646656583854\n",
      "PLPro_pocket6 0.2988809361086951\n",
      "PLPro_pocket50 0.41308884387581446\n"
     ]
    }
   ],
   "source": [
    "norm_mae=[]\n",
    "for i in range(18):\n",
    "    layer_dims = NN_arch[i]\n",
    "    model = Docking_Prediction(layer_dims)\n",
    "    model.load_state_dict(torch.load('BestNN_circ/bestNN_circ_col{}'.format(i)))#load the best model\n",
    "    preds = model(test_data).detach().numpy()\n",
    "\n",
    "    test_label=np.array(train[y_names[i]][test_idx])\n",
    "    print(y_names[i], metrics.mean_absolute_error(test_label, preds))\n",
    "    norm_mae.append(metrics.mean_absolute_error(test_label, preds)/(test_label.max()-test_label.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06545764691653773,\n",
       " 0.029667159915676675,\n",
       " 0.029141942487877687,\n",
       " 0.035141714752377685,\n",
       " 0.03483549454608965,\n",
       " 0.028975781446389754,\n",
       " 0.04326789403224317,\n",
       " 0.042862786267289225,\n",
       " 0.03364772140924224,\n",
       " 0.027699789384404486,\n",
       " 0.03335123383296269,\n",
       " 0.028916567803802404,\n",
       " 0.03161255336733645,\n",
       " 0.03902486514757295,\n",
       " 0.042709754344813285,\n",
       " 0.037941856123876544,\n",
       " 0.030498054704968885,\n",
       " 0.033584458851692235]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "layer_dims = NN_arch[i]\n",
    "model = Docking_Prediction(layer_dims)\n",
    "model.load_state_dict(torch.load('BestNN_circ/bestNN_circ_col{}'.format(i)))#load the best model\n",
    "preds_test = model(test_data).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,6])\n",
    "plt.plot(preds_test.detach().numpy(), np.array(train[y_names[col]][split_idx[0][1]]), '.')\n",
    "plt.xlabel('Predicted score', fontsize=14)\n",
    "plt.ylabel('True score', fontsize=14)\n",
    "plt.savefig('col0.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic3",
   "language": "python",
   "name": "mimic3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
